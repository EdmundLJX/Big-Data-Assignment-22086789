#Switch to the Hadoop user
root@ip-172-31-92-231:~# sudo su - hadoop


#Create a workspace and move into it
hadoop@ip-172-31-92-231:~$ mkdir assignmentbigdata
hadoop@ip-172-31-92-231:~$ cd assignmentbigdata


#Download dataset from S3 bucket to the EC2 local disk
hadoop@ip-172-31-92-231:~/assignmentbigdata$ wget https://bigdatadataset1.s3.us-east-1.amazonaws.com/JobSummary500000.csv


#List the files in the current directory
hadoop@ip-172-31-92-231:~/assignmentbigdata$ ls


#Start the Hadoop Distributed File System (HDFS) services
hadoop@ip-172-31-92-231:~/assignmentbigdata$ start-dfs.sh


#Start the YARN (Yet Another Resource Negotiator) services
hadoop@ip-172-31-92-231:~/assignmentbigdata$ start-yarn.sh


#List all running Java processes to verify that Hadoop daemons are active
hadoop@ip-172-31-92-231:~/assignmentbigdata$ jps


#Start the DataNode process manually if it is not active
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hadoop-daemon.sh start datanode


#Start the NodeManager process manually if it is not active
hadoop@ip-172-31-92-231:~/assignmentbigdata$ yarn-daemon.sh start nodemanager


#Create a directory named /input in HDFS to store the input file for the MapReduce job
#The -p flag ensures parent directories are created if they don't exist
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hdfs dfs -mkdir -p /user/hadoop/input


#Upload the local dataset file JobSummary500000.csv from the EC2 instance into HDFS
#Store it in the /input directory with the name job_count.csv
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hdfs dfs -put ~/assignmentbigdata/JobSummary500000.csv /user/hadoop/input/job_count.csv


#List the contents of the /input directory in HDFS
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hdfs dfs -ls /user/hadoop/input


#Open or create the WordCount.java file to write or edit the MapReduce program
hadoop@ip-172-31-92-231:~/assignmentbigdata$ nano WordCount.java


#MapReduce program code
import java.io.IOException;
import java.util.StringTokenizer;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 
public class WordCount {
  public static class TokenizerMapper
       extends Mapper<Object, Text, Text, IntWritable>{
 
    private final static IntWritable one = new IntWritable(1);
    private Text word = new Text();
 
    public void map(Object key, Text value, Context context
                    ) throws IOException, InterruptedException {
      StringTokenizer itr = new StringTokenizer(value.toString());
      while (itr.hasMoreTokens()) {
        word.set(itr.nextToken().toLowerCase().replaceAll("[^a-zA-Z]", ""));
        context.write(word, one);
      }
    }
  }
 
  public static class IntSumReducer
       extends Reducer<Text,IntWritable,Text,IntWritable> {
    private IntWritable result = new IntWritable();
 
    public void reduce(Text key, Iterable<IntWritable> values,
                       Context context
                       ) throws IOException, InterruptedException {
      int sum = 0;
      for (IntWritable val : values) {
        sum += val.get();
      }
      result.set(sum);
      context.write(key, result);
    }
  }
 
  public static void main(String[] args) throws Exception {
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf, "word count");
    job.setJarByClass(WordCount.class);
    job.setMapperClass(TokenizerMapper.class);
    job.setCombinerClass(IntSumReducer.class);
    job.setReducerClass(IntSumReducer.class);
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    FileInputFormat.addInputPath(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    System.exit(job.waitForCompletion(true) ? 0 : 1);
  }
}


#Compile the WordCount.java source file using the Hadoop classpath
hadoop@ip-172-31-92-231:~/assignmentbigdata$ javac -classpath "$(hadoop classpath)" WordCount.java


#Package all compiled WordCount*.class files into a JAR file named wordcount.jar
#This JAR will be used to run the MapReduce job in Hadoop
hadoop@ip-172-31-92-231:~/assignmentbigdata$ jar cf wordcount.jar WordCount*.class


#Remove existing output directory in HDFS
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hdfs dfs -rm -r -f /user/hadoop/output


#Run the MapReduce job using the compiled JAR (wordcount.jar)
#/user/hadoop/input  -> HDFS directory containing the input dataset
#/user/hadoop/output -> HDFS directory where the job results will be stored
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hadoop jar wordcount.jar WordCount /user/hadoop/input /user/hadoop/output


#List the contents of the output directory to verify the job output
#This is optional
hadoop@ip-172-31-92-231:~/assignmentbigdata$ hdfs dfs -ls /user/hadoop/output


#Merge HDFS output into a single local file
hdfs dfs -getmerge /user/hadoop/output output.txt


#Sort the local output.txt file in descending numerical order
#Then display only the top 10 most frequent words
hadoop@ip-172-31-92-231:~/assignmentbigdata$ sort -k2 -nr output.txt | head -n 10


#Calculate the total word count from the output file and display it
hadoop@ip-172-31-92-231:~/assignmentbigdata$ total_all=$(awk '{sum += $2} END {print sum}' output.txt)
echo "Total words in output.txt: $total_all"
echo ""


#Define a list of keywords related to jobs (such as technology, finance, etc.)
#For each word, count its occurrence in output.txt
hadoop@ip-172-31-92-231:~/assignmentbigdata$ for word in technology finance engineer nurse sales teacher manufacturing construction logistics recruitment
do
  total=$(grep -i "$word" output.txt | awk '{sum += $2} END {print sum}')
  echo "$word: $total"
done


#List the details of the local output.txt file
ls -lh output.txt


#Upload the final output.txt file to your S3 bucket for storage
hadoop@ip-172-31-92-231:~/assignmentbigdata$ aws s3 cp output.txt s3://bigdatadataset1/output.txt